---
title: Sparsity Regret Bounds for Individual Sequences in Online Linear Regression
abstract: We consider the problem of online linear regression on arbitrary deterministic
  sequences when the ambient dimension d can be much larger than the number of time
  rounds T. We introduce the notion of sparsity regret bound, which is a deterministic
  online counterpart of recent risk bounds derived in the stochastic setting under
  a sparsity scenario. We prove such regret bounds for an online-learning algorithm
  called SeqSEW and based on exponential weighting and data-driven truncation. In
  a second part we apply a parameter-free version of this algorithm on i.i.d. data
  and derive risk bounds of the same flavor as in \citetDaTsy08SEW,DaTsy10MirrorAveraging
  but which solve two questions left open therein. In particular our risk bounds are
  adaptive (up to a logarithmic factor) to the unknown variance of the noise if the
  latter is Gaussian.
pdf: http://proceedings.mlr.press/v19/gerchinovitz11a/gerchinovitz11a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: gerchinovitz11a
month: 0
tex_title: Sparsity Regret Bounds for Individual Sequences in Online Linear Regression
firstpage: 377
lastpage: 396
page: 377-396
order: 377
cycles: false
author:
- given: SÃ©bastien
  family: Gerchinovitz
date: 2011-12-21
address: Budapest, Hungary
publisher: PMLR
container-title: Proceedings of the 24th Annual Conference on Learning Theory
volume: '19'
genre: inproceedings
issued:
  date-parts:
  - 2011
  - 12
  - 21
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
