---
title: 'Beyond the regret minimization barrier: an optimal algorithm for stochastic
  strongly-convex optimization'
abstract: We give a novel algorithm for stochastic strongly-convex optimization in
  the gradient oracle model which returns an $O(\frac1T)$-approximate solution after $T$
  gradient updates. This rate of convergence is optimal in the gradientoracle model.
  This improves upon the previously known best rate of $O(\frac{\log(T)}{T})$, which was
  obtained by applying an online strongly-convex optimization algorithm with regret
  $O(\log(T))$ to the batch setting. We complement this result by proving that any algorithm
  has expected regret of $\Omega(\log(T))$ in the online stochastic strongly-convex optimization setting.
  This lower bound holds even in the full-information setting which reveals more information
  to the algorithm than just gradients. This shows that any online-to-batch conversion
  is inherently suboptimal for stochastic strongly-convex optimization. This is the
  first formal evidence that online convex optimization is strictly more difficult
  than batch stochastic convex optimization.
pdf: http://proceedings.mlr.press/v19/hazan11a/hazan11a.pdf
layout: inproceedings
series: Proceedings of Machine Learning Research
id: hazan11a
month: 0
tex_title: 'Beyond the regret minimization barrier: an optimal algorithm for stochastic
  strongly-convex optimization'
firstpage: 421
lastpage: 436
page: 421-436
order: 421
cycles: false
author:
- given: Elad
  family: Hazan
- given: Satyen
  family: Kale
date: 2011-12-21
address: Budapest, Hungary
publisher: PMLR
container-title: Proceedings of the 24th Annual Conference on Learning Theory
volume: '19'
genre: inproceedings
issued:
  date-parts:
  - 2011
  - 12
  - 21
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
