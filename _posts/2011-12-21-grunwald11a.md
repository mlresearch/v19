---
title: 'Safe Learning:    bridging the gap between Bayes, MDL and statistical learning
  theory via  empirical convexity'
abstract: We extend Bayesian MAP and Minimum Description Length (MDL) learning by
  testing whether the data can be substantially more compressed by a mixture of the
  MDL/MAP distribution with another element of themodel, and adjusting the learning
  rate if this is the case. While standard Bayes and MDL can fail to converge ifthe
  model is wrong, the resulting “safe” estimator continues toachieve good rates with
  wrong models. Moreover, when applied toclassification and regression models as considered
  in statisticallearning theory, the approach achieves optimal rates under, e.g.,Tsybakov’s
  conditions, and reveals new situations in which we canpenalize by (- \log \text\sc
  prior)/n rather than \sqrt(- \log \text\sc prior)/n.
pdf: http://jmlr.org/proceedings/papers/v19/grunwald11a/grunwald11a.pdf
layout: inproceedings
id: grunwald11a
month: 0
firstpage: 397
lastpage: 420
page: 397-420
sections: 
author:
- given: Peter
  family: Grünwald
reponame: v19
date: 2011-12-21
address: Budapest, Hungary
publisher: PMLR
container-title: Proceedings of the 24th Annual Conference on Learning Theory
volume: '19'
genre: inproceedings
issued:
  date-parts:
  - 2011
  - 12
  - 21
# Format based on citeproc: http://blog.martinfenner.org/2013/07/30/citeproc-yaml-for-bibliographies/
---
