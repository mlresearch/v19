---
title: On the Consistency of Multi-Label Learning
abstract: Multi-label learning has attracted much attention during the past few years.
  Many multi-label learning approaches have been developed, mostly working with surrogate
  loss functions since multi-label loss functions are usually difficult to optimize
  directly owing to non-convexity and discontinuity. Though these approaches are effective,
  to the best of our knowledge, there is no theoretical result on the convergence
  of risk of the learned functions to the Bayes risk. In this paper, focusing on two
  well-known multi-label loss functions, i.e., \textit{ranking loss} and \textit{hamming
  loss}, we prove a necessary and sufficient condition for the consistency of multi-label
  learning based on surrogate loss functions. Our results disclose that, surprisingly,
  none convex surrogate loss is consistent with the ranking loss. Inspired by the
  finding, we introduce the \textit{partial ranking loss}, with which some surrogate
  functions are consistent. For hamming loss, we show that some recent multi-label
  learning approaches are inconsistent even for deterministic multi-label classification,
  and give a surrogate loss function which is consistent for the deterministic case.
  Finally, we discuss on the consistency of learning approaches which address multi-label
  learning by decomposing into a set of binary classification problems.
pdf: "./gao11a/gao11a.pdf"
layout: inproceedings
id: gao11a
month: 0
firstpage: 341
lastpage: 358
page: 341-358
origpdf: http://jmlr.org/proceedings/papers/v19/gao11a/gao11a.pdf
sections: 
author:
- given: Wei
  family: Gao
- given: Zhi-Hua
  family: Zhou
date: '2011-12-21 00:05:41'
publisher: PMLR
---
