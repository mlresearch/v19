<html><head>
<meta http-equiv="content-type" content="text/html; charset=UTF-8">

<div id="content">
<h2>On the Consistency of Multi-Label Learning</h2>
<p><i><b>Wei Gao, Zhi-Hua Zhou</b></i> ; JMLR W&amp;CP 19:341-358, 2011.</p>

<h3>Abstract</h3>
Multi-label learning has attracted much attention during the past few years. Many multi-label learning approaches have been developed, mostly working with surrogate loss functions since multi-label loss functions are usually difficult to optimize directly owing to non-convexity and discontinuity. Though these approaches are effective, to the best of our knowledge, there is no theoretical result on the convergence of risk of the learned functions to the Bayes risk. In this paper, focusing on two well-known multi-label loss functions, i.e., \textit{ranking loss} and \textit{hamming loss}, we prove a necessary and sufficient condition for the consistency of multi-label learning based on surrogate loss functions. Our results disclose that, surprisingly, none convex surrogate loss is consistent with the ranking loss. Inspired by the finding, we introduce the \textit{partial ranking loss}, with which some surrogate functions are consistent. For hamming loss, we show that some recent multi-label learning approaches are inconsistent even for deterministic multi-label classification, and give a surrogate loss function which is consistent for the deterministic case. Finally, we discuss on the consistency of learning approaches which address multi-label learning by decomposing into a set of binary classification problems.
</div>

<hr>
<center>Page last modified on Sat Dec 17 01:06 CET 2011.</center>
